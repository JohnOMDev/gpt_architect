{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def12823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import pickle\n",
    "from review_card import ReviewCard\n",
    "from model import BigramLanguageModel\n",
    "from dataclasses import dataclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb6f054",
   "metadata": {},
   "outputs": [],
   "source": [
    "device ='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "review_handler = ReviewCard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb59171",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 64\n",
    "    batch_size:int = 256\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 2\n",
    "    n_head: int = 2\n",
    "    n_embd: int = 384\n",
    "    dropout: float = 0.2\n",
    "    learning_rate:float = 3e-4\n",
    "    max_iters:int = 200\n",
    "    eval_iters:int = 384\n",
    "    eval_interval:int = 100\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5f3094",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4965b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = review_handler.review_cralwer(page_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b371e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = '\\n'.join(row['message'] for row in text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72600d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding with tiktoken\n",
    "enc =tiktoken.get_encoding('gpt2')\n",
    "data_enc = torch.tensor(enc.encode_ordinary(data), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d432b842",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data_enc)) # 90% will be train, rest val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a9a5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = data_enc[:n]\n",
    "val = data_enc[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50edd19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split:str, config):\n",
    "    data = train if split == 'train' else val\n",
    "    ix = torch.randint(len(data)-config.block_size,(config.batch_size,))\n",
    "    x = torch.stack([data[i:i+ config.block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+config.block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8ee3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, config):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(config.eval_iters)\n",
    "        for k in range(config.eval_iters):\n",
    "            X,Y =get_batch(split, config)\n",
    "            logits, loss = model(X,Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] =losses.mean()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6067245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training():\n",
    "    config = GPTConfig()\n",
    "    model = BigramLanguageModel(config)\n",
    "    \n",
    "    # Create pytorch optimiser\n",
    "    optimizer =torch.optim.Adam(m.parameters(), lr=config.learning_rate)\n",
    "    \n",
    "    for iter in range(config.max_iters):\n",
    "        \n",
    "        # every oncein a while evaluate the loss on train and val sets\n",
    "        if iter % config.eval_interval ==0:\n",
    "            losses = estimate_loss(model, config)\n",
    "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "            \n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch(split = \"train\", config=config)\n",
    "        logits, loss =  model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0a2edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pkl_file = \"model-gpt-01.pkl\"\n",
    "model = training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd8bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_pkl_file, 'wb') as file:  \n",
    "    pickle.dump(model, file)\n",
    "m = model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
