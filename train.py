#!/usr/bin/env python3# -*- coding: utf-8 -*-"""Created on Tue Feb 1 11:59:01 2024@author: johnomole"""import sysimport torchimport tiktokenimport picklefrom review_card import ReviewCardfrom model import BigramLanguageModelfrom dataclasses import dataclassdevice ='cuda' if torch.cuda.is_available() else 'cpu'review_handler = ReviewCard()@dataclassclass GPTConfig:    block_size: int = 64    batch_size:int = 16    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency    n_layer: int = 4    n_head: int = 4    n_embd: int = 384    dropout: float = 0.2    learning_rate:float = 3e-2    max_iters:int = 1000    eval_iters:int = 384    eval_interval:int = 200    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and fastertorch.manual_seed(1337)text = review_handler.review_cralwer(page_size=20)#unique character that occur in this text# =============================================================================# chars=sorted(list(set(text)))# vocab_size = len(chars)# =============================================================================data  = '\n'.join(str(row['message']) for row in text)# encoding with tiktokenenc =tiktoken.get_encoding('gpt2')data_enc = torch.tensor(enc.encode_ordinary(data), dtype=torch.long)# =============================================================================# train = enc.encode_ordinary(data[:n])# val = enc.encode_ordinary(data[n:])# =============================================================================n = int(0.9 * len(data_enc)) # 90% will be train, rest valtrain = data_enc[:n]val = data_enc[n:]# data loadingdef get_batch(split:str, config):    data = train if split == 'train' else val    ix = torch.randint(len(data)-config.block_size,(config.batch_size,))    x = torch.stack([data[i:i+ config.block_size] for i in ix])    y = torch.stack([data[i+1:i+config.block_size+1] for i in ix])    x,y = x.to(device), y.to(device)    return x,y@torch.no_grad()def estimate_loss(model, config):    out = {}    model.eval()    for split in ['train', 'val']:        losses = torch.zeros(config.eval_iters)        for k in range(config.eval_iters):            X,Y =get_batch(split, config)            logits, loss = model(X,Y)            losses[k] = loss.item()        out[split] =losses.mean()    model.train()    return outdef training():    config = GPTConfig(**config_meta)    model = BigramLanguageModel(config)    m = model.to(device)    # Create pytorch optimiser    optimizer =torch.optim.Adam(model.parameters(), lr=config.learning_rate)        for iter in range(config.max_iters):                # every oncein a while evaluate the loss on train and val sets        if iter % config.eval_interval ==0 or iter == config.max_iters -1:            losses = estimate_loss(model, config)            print(f"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}")                    # sample a batch of data        xb, yb = get_batch(split = "train", config=config)        logits, loss =  model(xb, yb)        optimizer.zero_grad(set_to_none=True)        loss.backward()        optimizer.step()    return mif __name__ == "__main__":    batch_size = sys.argv[1]    config_meta = {}    config_meta['batch_size'] = batch_size if batch_size else 16    model_pkl_file = "model-gpt-01.pkl"    m = training()    # Save the model    with open(model_pkl_file, 'wb') as file:          pickle.dump(m, file)